{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e45d2062",
   "metadata": {},
   "source": [
    "# Análisis de Regresión: Predicción de Consumo de Combustible en Automóviles\n",
    "\n",
    "## Objetivo del Proyecto\n",
    "Este notebook presenta un análisis completo de regresión para predecir el consumo de combustible (MPG - Millas por Galón) de automóviles basándose en características técnicas como cilindros, desplazamiento, peso, etc.\n",
    "\n",
    "## Metodología\n",
    "- Análisis exploratorio de datos (EDA)\n",
    "- Preprocesamiento y limpieza de datos\n",
    "- Normalización de variables\n",
    "- Implementación de modelos de regresión\n",
    "- Evaluación y comparación de resultados\n",
    "\n",
    "---\n",
    "*Desarrollado como parte del curso de Machine Learning - Análisis de Datos*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94368926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Importación de librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Carga del dataset de automóviles\n",
    "# Este dataset contiene información sobre el consumo de combustible y características técnicas\n",
    "print(\"📊 Cargando dataset de automóviles...\")\n",
    "dataset = pd.read_csv(\"../files/input/auto_mpg.csv\")\n",
    "\n",
    "print(f\"✅ Dataset cargado exitosamente\")\n",
    "print(f\"📈 Dimensiones: {dataset.shape[0]} filas x {dataset.shape[1]} columnas\")\n",
    "print(\"\\n🔍 Primeras 5 filas del dataset:\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553de8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Análisis de la estructura del dataset\n",
    "print(\"📏 INFORMACIÓN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"📊 Dimensiones: {dataset.shape[0]} filas x {dataset.shape[1]} columnas\")\n",
    "print(f\"💾 Memoria utilizada: {dataset.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"🏷️  Columnas disponibles: {list(dataset.columns)}\")\n",
    "print(\"\\n📋 Información detallada de tipos de datos:\")\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7ecf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPG             0\n",
       "Cylinders       0\n",
       "Displacement    0\n",
       "Horsepower      6\n",
       "Weight          0\n",
       "Acceleration    0\n",
       "Model Year      0\n",
       "Origin          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Análisis de valores faltantes (Missing Values)\n",
    "print(\"🔍 ANÁLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Contar valores nulos por columna\n",
    "missing_values = dataset.isna().sum()\n",
    "missing_percentage = (missing_values / len(dataset)) * 100\n",
    "\n",
    "# Crear DataFrame para mejor visualización\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores_Faltantes': missing_values,\n",
    "    'Porcentaje': missing_percentage\n",
    "})\n",
    "\n",
    "print(\"📊 Resumen de valores faltantes por columna:\")\n",
    "print(missing_df[missing_df['Valores_Faltantes'] > 0])\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"✅ ¡Excelente! No se encontraron valores faltantes en el dataset\")\n",
    "else:\n",
    "    print(f\"⚠️  Total de valores faltantes: {missing_values.sum()}\")\n",
    "    print(f\"📈 Porcentaje total de datos faltantes: {(missing_values.sum() / (dataset.shape[0] * dataset.shape[1])) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1054dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPG             0\n",
       "Cylinders       0\n",
       "Displacement    0\n",
       "Horsepower      0\n",
       "Weight          0\n",
       "Acceleration    0\n",
       "Model Year      0\n",
       "Origin          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limpieza de datos: Eliminación de registros con valores faltantes\n",
    "print(\"🧹 LIMPIEZA DE DATOS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Guardar el tamaño original para comparación\n",
    "original_size = dataset.shape[0]\n",
    "\n",
    "# Eliminar filas con valores nulos\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Calcular el impacto de la limpieza\n",
    "final_size = dataset.shape[0]\n",
    "removed_rows = original_size - final_size\n",
    "\n",
    "print(f\"📊 Tamaño original: {original_size} filas\")\n",
    "print(f\"📊 Tamaño después de limpieza: {final_size} filas\")\n",
    "print(f\"🗑️  Filas eliminadas: {removed_rows} ({(removed_rows/original_size)*100:.2f}%)\")\n",
    "\n",
    "# Verificar que no quedan valores nulos\n",
    "print(f\"\\n✅ Verificación final - Valores faltantes restantes:\")\n",
    "print(dataset.isna().sum().sum())\n",
    "\n",
    "print(f\"\\n🎯 Dataset final listo para análisis: {dataset.shape[0]} filas x {dataset.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa5666f",
   "metadata": {},
   "source": [
    "## 🏭 Análisis de la Variable Categórica \"Origin\"\n",
    "\n",
    "La columna `Origin` representa el país de origen del automóvil, pero está codificada numéricamente:\n",
    "- **1** = USA (Estados Unidos)\n",
    "- **2** = Europe (Europa) \n",
    "- **3** = Japan (Japón)\n",
    "\n",
    "**Problema identificado:** Al ser una variable categórica, no tiene sentido matemático mantenerla como valor numérico para el modelo de regresión. Los algoritmos de machine learning interpretarían incorrectamente que \"Japón (3)\" es \"3 veces más importante\" que \"USA (1)\", cuando en realidad son categorías independientes.\n",
    "\n",
    "**Solución:** Convertiremos esta variable a formato categórico y luego la transformaremos en variables dummy (one-hot encoding) para que el modelo pueda procesarla correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf9899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Origin\n",
       "1    245\n",
       "3     79\n",
       "2     68\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Análisis de la distribución de países de origen\n",
    "print(\"🌍 DISTRIBUCIÓN DE PAÍSES DE ORIGEN\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Mostrar conteo de valores\n",
    "origin_counts = dataset.Origin.value_counts()\n",
    "print(\"📊 Conteo de automóviles por país:\")\n",
    "print(origin_counts)\n",
    "\n",
    "# Calcular porcentajes\n",
    "print(f\"\\n📈 Distribución porcentual:\")\n",
    "for country_code, count in origin_counts.items():\n",
    "    country_name = {1: \"USA\", 2: \"Europe\", 3: \"Japan\"}[country_code]\n",
    "    percentage = (count / len(dataset)) * 100\n",
    "    print(f\"  {country_name} ({country_code}): {count} vehículos ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "origin_counts.plot(kind='bar', color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "plt.title('Distribución de Países de Origen')\n",
    "plt.xlabel('Código de País')\n",
    "plt.ylabel('Cantidad de Vehículos')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(origin_counts.values, labels=['USA', 'Europe', 'Japan'], autopct='%1.1f%%', \n",
    "        colors=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "plt.title('Proporción de Países de Origen')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ad42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de variable categórica numérica a texto\n",
    "print(\"🔄 TRANSFORMACIÓN DE VARIABLE CATEGÓRICA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Mapear códigos numéricos a nombres de países\n",
    "country_mapping = {1: \"USA\", 2: \"Europe\", 3: \"Japan\"}\n",
    "dataset[\"Origin\"] = dataset[\"Origin\"].map(country_mapping)\n",
    "\n",
    "print(\"✅ Variable 'Origin' convertida a formato categórico\")\n",
    "print(f\"📊 Valores únicos: {dataset['Origin'].unique()}\")\n",
    "print(f\"📈 Distribución actualizada:\")\n",
    "print(dataset[\"Origin\"].value_counts())\n",
    "\n",
    "# Nota importante sobre buenas prácticas\n",
    "print(f\"\\n⚠️  NOTA IMPORTANTE:\")\n",
    "print(f\"En un entorno de producción, este mapeo debería estar:\")\n",
    "print(f\"• Definido en un archivo de configuración\")\n",
    "print(f\"• Validado contra un diccionario de datos\")\n",
    "print(f\"• Incluir manejo de valores inesperados\")\n",
    "print(f\"• Ser versionado y documentado apropiadamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0bf911",
   "metadata": {},
   "source": [
    "## ⚠️ Consideraciones Importantes sobre One-Hot Encoding\n",
    "\n",
    "### Problemas en Producción:\n",
    "1. **Dependencia de valores fijos**: El modelo entrenado solo reconoce las categorías exactas que vio durante el entrenamiento\n",
    "2. **Fragilidad ante nuevos datos**: Si aparecen nuevas categorías (ej: \"China\", \"India\"), el modelo fallará\n",
    "3. **Inconsistencia de columnas**: El número de columnas dummy debe ser idéntico entre entrenamiento y predicción\n",
    "4. **Mantenimiento complejo**: Cualquier cambio en las categorías requiere retrenar el modelo\n",
    "\n",
    "### Alternativas más robustas:\n",
    "- **Label Encoding** con manejo de categorías desconocidas\n",
    "- **Embeddings** para variables categóricas con muchas categorías\n",
    "- **Feature hashing** para categorías dinámicas\n",
    "- **Validación estricta** de datos de entrada en producción\n",
    "\n",
    "### Para este ejercicio académico:\n",
    "Usaremos One-Hot Encoding por simplicidad, pero siempre considerando estas limitaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc0d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Japan</th>\n",
       "      <th>USA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5   \n",
       "\n",
       "   Model Year  Europe  Japan   USA  \n",
       "0          70   False  False  True  \n",
       "1          70   False  False  True  \n",
       "2          70   False  False  True  \n",
       "3          70   False  False  True  \n",
       "4          70   False  False  True  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicación de One-Hot Encoding para variables categóricas\n",
    "print(\"🔧 APLICACIÓN DE ONE-HOT ENCODING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Guardar el estado antes de la transformación\n",
    "print(f\"📊 Columnas antes de One-Hot Encoding: {list(dataset.columns)}\")\n",
    "print(f\"📏 Dimensiones antes: {dataset.shape}\")\n",
    "\n",
    "# Aplicar One-Hot Encoding a la columna Origin\n",
    "dataset = pd.get_dummies(dataset, columns=[\"Origin\"], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "print(f\"\\n✅ One-Hot Encoding aplicado exitosamente\")\n",
    "print(f\"📊 Columnas después de One-Hot Encoding: {list(dataset.columns)}\")\n",
    "print(f\"📏 Dimensiones después: {dataset.shape}\")\n",
    "\n",
    "# Mostrar las nuevas columnas dummy\n",
    "dummy_columns = [col for col in dataset.columns if col in ['USA', 'Europe', 'Japan']]\n",
    "print(f\"\\n🏷️  Nuevas columnas dummy creadas: {dummy_columns}\")\n",
    "\n",
    "# Verificar que cada fila tenga exactamente un 1 en las columnas dummy\n",
    "print(f\"\\n🔍 Verificación de consistencia:\")\n",
    "for col in dummy_columns:\n",
    "    print(f\"  {col}: {dataset[col].sum()} vehículos\")\n",
    "\n",
    "print(f\"\\n📋 Primeras 5 filas del dataset transformado:\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División del dataset en conjuntos de entrenamiento y prueba\n",
    "print(\"📊 DIVISIÓN DEL DATASET\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Configuración de la división\n",
    "train_fraction = 0.8\n",
    "random_seed = 42  # Cambiado de 0 a 42 para mejor reproducibilidad\n",
    "\n",
    "print(f\"🎯 Configuración de división:\")\n",
    "print(f\"  • Fracción para entrenamiento: {train_fraction*100}%\")\n",
    "print(f\"  • Fracción para prueba: {(1-train_fraction)*100}%\")\n",
    "print(f\"  • Semilla aleatoria: {random_seed}\")\n",
    "\n",
    "# División usando muestreo aleatorio estratificado\n",
    "train_dataset = dataset.sample(frac=train_fraction, random_state=random_seed)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "# Verificar la división\n",
    "train_size = len(train_dataset)\n",
    "test_size = len(test_dataset)\n",
    "total_size = train_size + test_size\n",
    "\n",
    "print(f\"\\n📈 Resultados de la división:\")\n",
    "print(f\"  • Dataset original: {total_size} registros\")\n",
    "print(f\"  • Conjunto de entrenamiento: {train_size} registros ({train_size/total_size*100:.1f}%)\")\n",
    "print(f\"  • Conjunto de prueba: {test_size} registros ({test_size/total_size*100:.1f}%)\")\n",
    "\n",
    "# Verificar que no hay solapamiento\n",
    "overlap = len(set(train_dataset.index) & set(test_dataset.index))\n",
    "print(f\"  • Solapamiento entre conjuntos: {overlap} (debe ser 0)\")\n",
    "\n",
    "print(f\"\\n⚠️  NOTA: Se usa muestreo aleatorio simple en lugar de train_test_split\")\n",
    "print(f\"    Esto es válido para este ejercicio, pero en producción se recomienda\")\n",
    "print(f\"    usar train_test_split de sklearn para mayor control y validación\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283dc4c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Las millas por galon (MPG) son función de las demas variables.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m sns.pairplot(\n\u001b[32m      7\u001b[39m     train_dataset[[\u001b[33m\"\u001b[39m\u001b[33mMPG\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCylinders\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDisplacement\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWeight\u001b[39m\u001b[33m\"\u001b[39m]], diag_kind=\u001b[33m\"\u001b[39m\u001b[33mkde\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Análisis Exploratorio de Datos (EDA) - Relaciones entre variables\n",
    "print(\"🔍 ANÁLISIS EXPLORATORIO DE DATOS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Seleccionar variables numéricas clave para el análisis\n",
    "numeric_vars = [\"MPG\", \"Cylinders\", \"Displacement\", \"Weight\"]\n",
    "print(f\"📊 Variables analizadas: {numeric_vars}\")\n",
    "\n",
    "# Crear visualización de relaciones entre variables\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Pairplot para visualizar relaciones\n",
    "sns.pairplot(\n",
    "    train_dataset[numeric_vars], \n",
    "    diag_kind=\"kde\",\n",
    "    plot_kws={'alpha': 0.6, 's': 20},\n",
    "    diag_kws={'alpha': 0.7}\n",
    ")\n",
    "\n",
    "plt.suptitle('Análisis de Relaciones entre Variables Clave', y=1.02, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis de correlaciones\n",
    "print(f\"\\n📈 MATRIZ DE CORRELACIONES:\")\n",
    "correlation_matrix = train_dataset[numeric_vars].corr()\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Identificar correlaciones fuertes con MPG\n",
    "mpg_correlations = correlation_matrix['MPG'].abs().sort_values(ascending=False)\n",
    "print(f\"\\n🎯 Correlaciones con MPG (valor absoluto):\")\n",
    "for var, corr in mpg_correlations.items():\n",
    "    if var != 'MPG':\n",
    "        strength = \"Fuerte\" if corr > 0.7 else \"Moderada\" if corr > 0.3 else \"Débil\"\n",
    "        print(f\"  • {var}: {corr:.3f} ({strength})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca95e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas descriptivas del conjunto de entrenamiento\n",
    "print(\"📊 ESTADÍSTICAS DESCRIPTIVAS - CONJUNTO DE ENTRENAMIENTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calcular estadísticas descriptivas\n",
    "stats = train_dataset.describe().transpose()\n",
    "\n",
    "# Agregar información adicional\n",
    "stats['rango'] = stats['max'] - stats['min']\n",
    "stats['coef_variacion'] = (stats['std'] / stats['mean']).round(3)\n",
    "\n",
    "print(\"📈 Resumen estadístico completo:\")\n",
    "print(stats.round(3))\n",
    "\n",
    "# Análisis de la variable objetivo (MPG)\n",
    "print(f\"\\n🎯 ANÁLISIS DE LA VARIABLE OBJETIVO (MPG):\")\n",
    "mpg_stats = train_dataset['MPG']\n",
    "print(f\"  • Rango: {mpg_stats.min():.1f} - {mpg_stats.max():.1f} MPG\")\n",
    "print(f\"  • Media: {mpg_stats.mean():.1f} MPG\")\n",
    "print(f\"  • Mediana: {mpg_stats.median():.1f} MPG\")\n",
    "print(f\"  • Desviación estándar: {mpg_stats.std():.1f} MPG\")\n",
    "print(f\"  • Coeficiente de variación: {(mpg_stats.std()/mpg_stats.mean())*100:.1f}%\")\n",
    "\n",
    "# Identificar variables con alta variabilidad\n",
    "print(f\"\\n⚠️  VARIABLES CON ALTA VARIABILIDAD (CV > 50%):\")\n",
    "high_var = stats[stats['coef_variacion'] > 0.5]\n",
    "if len(high_var) > 0:\n",
    "    for var in high_var.index:\n",
    "        print(f\"  • {var}: CV = {high_var.loc[var, 'coef_variacion']*100:.1f}%\")\n",
    "else:\n",
    "    print(\"  ✅ No se detectaron variables con variabilidad excesiva\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a338ade",
   "metadata": {},
   "source": [
    "## 🎯 Definición de la Variable Objetivo\n",
    "\n",
    "**MPG (Miles Per Gallon)** será nuestra variable de salida (target) que queremos predecir.\n",
    "\n",
    "### ¿Por qué MPG es importante?\n",
    "- **Eficiencia energética**: Indica qué tan eficiente es un vehículo en el consumo de combustible\n",
    "- **Impacto económico**: Afecta directamente los costos de operación del vehículo\n",
    "- **Sostenibilidad**: Relacionado con las emisiones de CO₂ y el impacto ambiental\n",
    "- **Regulación**: Muchos países tienen estándares mínimos de eficiencia de combustible\n",
    "\n",
    "### Objetivo del modelo:\n",
    "Desarrollar un modelo de regresión que pueda predecir el consumo de combustible (MPG) basándose en las características técnicas del vehículo, lo que permitiría:\n",
    "- Optimizar el diseño de vehículos\n",
    "- Estimar el consumo antes de la fabricación\n",
    "- Clasificar vehículos por eficiencia energética\n",
    "- Apoyar decisiones de compra informadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea95cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de características (features) y etiquetas (labels)\n",
    "print(\"🔀 SEPARACIÓN DE CARACTERÍSTICAS Y ETIQUETAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear copias para evitar modificar los datasets originales\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "# Extraer la variable objetivo (MPG) de ambos conjuntos\n",
    "train_labels = train_features.pop(\"MPG\")\n",
    "test_labels = test_features.pop(\"MPG\")\n",
    "\n",
    "print(\"✅ Separación completada exitosamente\")\n",
    "print(f\"\\n📊 Conjunto de entrenamiento:\")\n",
    "print(f\"  • Características: {train_features.shape[0]} filas x {train_features.shape[1]} columnas\")\n",
    "print(f\"  • Etiquetas: {len(train_labels)} valores\")\n",
    "print(f\"  • Rango de MPG: {train_labels.min():.1f} - {train_labels.max():.1f}\")\n",
    "\n",
    "print(f\"\\n📊 Conjunto de prueba:\")\n",
    "print(f\"  • Características: {test_features.shape[0]} filas x {test_features.shape[1]} columnas\")\n",
    "print(f\"  • Etiquetas: {len(test_labels)} valores\")\n",
    "print(f\"  • Rango de MPG: {test_labels.min():.1f} - {test_labels.max():.1f}\")\n",
    "\n",
    "# Verificar que las dimensiones coincidan\n",
    "assert train_features.shape[0] == len(train_labels), \"Error: dimensiones no coinciden en entrenamiento\"\n",
    "assert test_features.shape[0] == len(test_labels), \"Error: dimensiones no coinciden en prueba\"\n",
    "\n",
    "print(f\"\\n✅ Verificación de consistencia: Dimensiones correctas\")\n",
    "print(f\"🏷️  Variables predictoras: {list(train_features.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a9248",
   "metadata": {},
   "source": [
    "## ⚖️ Necesidad de Normalización de Datos\n",
    "\n",
    "### Problema identificado:\n",
    "Las variables del dataset tienen **escalas muy diferentes**:\n",
    "- **Cilindros**: 3-8 (escala pequeña)\n",
    "- **Desplazamiento**: 68-455 (escala media)\n",
    "- **Peso**: 1613-5140 (escala grande)\n",
    "- **Año**: 70-82 (escala temporal)\n",
    "\n",
    "### ¿Por qué es problemático?\n",
    "1. **Algoritmos sensibles a escala**: Algoritmos como regresión lineal, SVM, redes neuronales son sensibles a la escala\n",
    "2. **Convergencia lenta**: El gradiente descendente puede converger muy lentamente\n",
    "3. **Sesgo hacia variables grandes**: Variables con valores más grandes dominan el modelo\n",
    "4. **Inestabilidad numérica**: Puede causar problemas de precisión en cálculos\n",
    "\n",
    "### Solución: StandardScaler\n",
    "Transformaremos todas las variables para que tengan:\n",
    "- **Media = 0**\n",
    "- **Desviación estándar = 1**\n",
    "\n",
    "Esto garantiza que todas las variables contribuyan equitativamente al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de escalas antes de la normalización\n",
    "print(\"📏 ANÁLISIS DE ESCALAS ANTES DE NORMALIZACIÓN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mostrar media y desviación estándar de las variables numéricas\n",
    "numeric_features = train_features.select_dtypes(include=[np.number]).columns\n",
    "pre_scaling_stats = train_features[numeric_features].describe().transpose()[[\"mean\", \"std\"]]\n",
    "\n",
    "print(\"📊 Estadísticas antes del escalado:\")\n",
    "print(pre_scaling_stats.round(3))\n",
    "\n",
    "# Calcular el rango de cada variable\n",
    "pre_scaling_stats['rango'] = train_features[numeric_features].max() - train_features[numeric_features].min()\n",
    "pre_scaling_stats['coef_variacion'] = (pre_scaling_stats['std'] / pre_scaling_stats['mean']).round(3)\n",
    "\n",
    "print(f\"\\n📈 Análisis de variabilidad:\")\n",
    "print(pre_scaling_stats[['rango', 'coef_variacion']].round(3))\n",
    "\n",
    "# Identificar variables con mayor impacto potencial\n",
    "print(f\"\\n⚠️  Variables con mayor rango (potencial dominancia):\")\n",
    "high_range = pre_scaling_stats['rango'].sort_values(ascending=False)\n",
    "for var, rango in high_range.items():\n",
    "    print(f\"  • {var}: rango = {rango:.1f}\")\n",
    "\n",
    "print(f\"\\n🎯 Justificación para normalización:\")\n",
    "print(f\"  • Diferencia de rangos: {high_range.max() / high_range.min():.1f}x\")\n",
    "print(f\"  • Variables con CV > 50%: {len(pre_scaling_stats[pre_scaling_stats['coef_variacion'] > 0.5])}\")\n",
    "print(f\"  • Necesidad de escalado: {'SÍ' if high_range.max() / high_range.min() > 10 else 'NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e412833",
   "metadata": {},
   "source": [
    "## 🎯 Objetivo de la Normalización\n",
    "\n",
    "### Transformación objetivo:\n",
    "- **Media = 0** (centrado)\n",
    "- **Desviación estándar = 1** (escalado)\n",
    "\n",
    "### Fórmula del StandardScaler:\n",
    "```\n",
    "z = (x - μ) / σ\n",
    "```\n",
    "Donde:\n",
    "- `x` = valor original\n",
    "- `μ` = media de la variable\n",
    "- `σ` = desviación estándar de la variable\n",
    "- `z` = valor normalizado\n",
    "\n",
    "### Consideraciones técnicas:\n",
    "- **Precisión decimal**: Los computadores tienen limitaciones de precisión, por lo que los valores exactos (0.0, 1.0) pueden tener pequeñas desviaciones\n",
    "- **Tolerancia**: Valores como 0.0001 o 0.9999 son aceptables\n",
    "- **Consistencia**: Lo importante es que todas las variables estén en la misma escala relativa\n",
    "\n",
    "### Beneficios esperados:\n",
    "✅ Convergencia más rápida del algoritmo  \n",
    "✅ Mejor estabilidad numérica  \n",
    "✅ Contribución equitativa de todas las variables  \n",
    "✅ Mejor interpretabilidad de coeficientes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b3100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicación del StandardScaler para normalización\n",
    "print(\"⚖️ APLICACIÓN DE STANDARDSCALER\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Importar y crear el scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"🔧 Configurando StandardScaler...\")\n",
    "print(\"  • Método: Z-score normalization\")\n",
    "print(\"  • Fórmula: z = (x - μ) / σ\")\n",
    "print(\"  • Aplicación: Solo en conjunto de entrenamiento\")\n",
    "\n",
    "# Aplicar el escalado al conjunto de entrenamiento\n",
    "print(f\"\\n📊 Aplicando normalización al conjunto de entrenamiento...\")\n",
    "train_scaled = scaler.fit_transform(train_features[numeric_features])\n",
    "\n",
    "# Crear DataFrame con los datos normalizados\n",
    "train_scaled_df = pd.DataFrame(\n",
    "    data=train_scaled,\n",
    "    columns=numeric_features,\n",
    "    index=train_features.index\n",
    ")\n",
    "\n",
    "# Agregar las columnas categóricas (que no necesitan escalado)\n",
    "for col in train_features.columns:\n",
    "    if col not in numeric_features:\n",
    "        train_scaled_df[col] = train_features[col]\n",
    "\n",
    "print(f\"✅ Normalización completada\")\n",
    "print(f\"📏 Dimensiones del dataset normalizado: {train_scaled_df.shape}\")\n",
    "\n",
    "# Verificar las estadísticas después del escalado\n",
    "print(f\"\\n📈 VERIFICACIÓN DE NORMALIZACIÓN:\")\n",
    "post_scaling_stats = train_scaled_df[numeric_features].describe().transpose()[[\"mean\", \"std\"]]\n",
    "print(\"Estadísticas después del escalado:\")\n",
    "print(post_scaling_stats.round(6))\n",
    "\n",
    "# Verificar que la media esté cerca de 0 y la desviación estándar cerca de 1\n",
    "print(f\"\\n🎯 Verificación de objetivos:\")\n",
    "for col in numeric_features:\n",
    "    mean_val = post_scaling_stats.loc[col, 'mean']\n",
    "    std_val = post_scaling_stats.loc[col, 'std']\n",
    "    mean_ok = abs(mean_val) < 0.01\n",
    "    std_ok = abs(std_val - 1.0) < 0.01\n",
    "    status = \"✅\" if (mean_ok and std_ok) else \"⚠️\"\n",
    "    print(f\"  {status} {col}: μ={mean_val:.6f}, σ={std_val:.6f}\")\n",
    "\n",
    "print(f\"\\n📋 Primeras 5 filas del dataset normalizado:\")\n",
    "train_scaled_df.head()\n",
    "\n",
    "print(\"🔧 Configurando StandardScaler...\")\n",
    "print(\"  • Método: Z-score normalization\")\n",
    "print(\"  • Fórmula: z = (x - μ) / σ\")\n",
    "print(\"  • Aplicación: Solo en conjunto de entrenamiento\")\n",
    "\n",
    "# Aplicar el escalado al conjunto de entrenamiento\n",
    "print(f\"\\n📊 Aplicando normalización al conjunto de entrenamiento...\")\n",
    "train_scaled = scaler.fit_transform(train_features[numeric_features])\n",
    "\n",
    "# Crear DataFrame con los datos normalizados\n",
    "train_scaled_df = pd.DataFrame(\n",
    "    data=train_scaled,\n",
    "    columns=numeric_features,\n",
    "    index=train_features.index\n",
    ")\n",
    "\n",
    "# Agregar las columnas categóricas (que no necesitan escalado)\n",
    "for col in train_features.columns:\n",
    "    if col not in numeric_features:\n",
    "        train_scaled_df[col] = train_features[col]\n",
    "\n",
    "print(f\"✅ Normalización completada\")\n",
    "print(f\"📏 Dimensiones del dataset normalizado: {train_scaled_df.shape}\")\n",
    "\n",
    "# Verificar las estadísticas después del escalado\n",
    "print(f\"\\n📈 VERIFICACIÓN DE NORMALIZACIÓN:\")\n",
    "post_scaling_stats = train_scaled_df[numeric_features].describe().transpose()[[\"mean\", \"std\"]]\n",
    "print(\"Estadísticas después del escalado:\")\n",
    "print(post_scaling_stats.round(6))\n",
    "\n",
    "# Verificar que la media esté cerca de 0 y la desviación estándar cerca de 1\n",
    "print(f\"\\n🎯 Verificación de objetivos:\")\n",
    "for col in numeric_features:\n",
    "    mean_val = post_scaling_stats.loc[col, 'mean']\n",
    "    std_val = post_scaling_stats.loc[col, 'std']\n",
    "    mean_ok = abs(mean_val) < 0.01\n",
    "    std_ok = abs(std_val - 1.0) < 0.01\n",
    "    status = \"✅\" if (mean_ok and std_ok) else \"⚠️\"\n",
    "    print(f\"  {status} {col}: μ={mean_val:.6f}, σ={std_val:.6f}\")\n",
    "\n",
    "print(f\"\\n📋 Primeras 5 filas del dataset normalizado:\")\n",
    "train_scaled_df.head()\n",
    ").describe().transpose()[[\"mean\", \"std\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd076b2",
   "metadata": {},
   "source": [
    "La transformación debe ser consistente, por lo tanto, hicimos el *fit* del escalador solamente sobre los datos de entrenamiento y lo aplicamos exactamente igual sobre los datos de prueba. De esta manera, tanto el dataset de train como el de test deben tener *media* aproximadamente igual a *0* y *desviación estándar* aproximadamente igual a *1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57627311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Preparación de la data\n",
    "#\n",
    "horsepower_scaler = StandardScaler()\n",
    "\n",
    "train_horsepower = train_features[[\"Horsepower\"]]\n",
    "test_horsepower = test_features[[\"Horsepower\"]]\n",
    "\n",
    "horsepower_scaler.fit(train_horsepower)\n",
    "\n",
    "standarized_train_horsepower = horsepower_scaler.transform(train_horsepower)\n",
    "standarized_test_horsepower = horsepower_scaler.transform(test_horsepower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d39e4",
   "metadata": {},
   "source": [
    "## Primer modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408670ea",
   "metadata": {},
   "source": [
    "Regresión lineal: Horsepower vs MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Modelo de regresión lineal\n",
    "#\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "horsepower_model = LinearRegression()\n",
    "horsepower_model.fit(standarized_train_horsepower, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa85b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Intercepto\n",
    "#\n",
    "horsepower_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91087c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Coeficientes\n",
    "#\n",
    "horsepower_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Predicción. Preparación de las variables independientes\n",
    "#\n",
    "import numpy as np\n",
    "\n",
    "x = pd.DataFrame({\"Horsepower\": np.linspace(0, 250, 251)})\n",
    "x.head(), x.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Predicción\n",
    "#\n",
    "scaled_x = horsepower_scaler.transform(x)\n",
    "y = horsepower_model.predict(scaled_x)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_horsepower(x, y):\n",
    "    plt.scatter(train_features[\"Horsepower\"], train_labels, label=\"Data\")\n",
    "    plt.plot(x, y, color=\"k\", label=\"Predictions\")\n",
    "    plt.xlabel(\"Horsepower\")\n",
    "    plt.ylabel(\"MPG\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horsepower(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75591b4",
   "metadata": {},
   "source": [
    "En los resultados de nuestro modelo, como es posible notar, el error cuadrático medio es muy alto y una posible explicación es que relación entre las variables en realidad no sea lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9999eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluación\n",
    "#\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "y_pred = horsepower_model.predict(standarized_test_horsepower)\n",
    "\n",
    "test_results[\"horsepower_model\"] = mean_squared_error(\n",
    "    y_true=test_labels,\n",
    "    y_pred=y_pred,\n",
    ")\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6bfca",
   "metadata": {},
   "source": [
    "## Segundo modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d4134",
   "metadata": {},
   "source": [
    "Regresión lineal: variables independientes vs MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15805d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Preparación de la data\n",
    "#\n",
    "features_scaler = StandardScaler()\n",
    "\n",
    "features_scaler.fit(train_features)\n",
    "\n",
    "standarized_train_features = features_scaler.transform(train_features)\n",
    "standarized_test_features = features_scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b07e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(standarized_train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Intercepto\n",
    "#\n",
    "linear_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829fe6b",
   "metadata": {},
   "source": [
    "Ahora tengo un coeficiente por cada variable de las *X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Coeficientes\n",
    "#\n",
    "linear_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_true, y_pred):\n",
    "\n",
    "    ax = plt.axes(aspect=\"equal\")\n",
    "    plt.scatter(y_true, y_pred)\n",
    "    plt.xlabel(\"True Values [MPG]\")\n",
    "    plt.ylabel(\"Predictions [MPG]\")\n",
    "    lims = [0, 50]\n",
    "    plt.xlim(lims)\n",
    "    plt.ylim(lims)\n",
    "    _ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b2cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = linear_model.predict(standarized_test_features)\n",
    "\n",
    "plot_predictions(\n",
    "    y_true=test_labels,\n",
    "    y_pred=test_predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41980d82",
   "metadata": {},
   "source": [
    "Como podemos observar, el nuevo modelo en el que introdujimos todas las variables independientes es mejor que el primer modelo en el que teníamos solamente la variable \"Horsepower\": el error cuadrático medio es mucho más bajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[\"linear_model\"] = mean_squared_error(\n",
    "    y_true=test_labels,\n",
    "    y_pred=test_predictions,\n",
    ")\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d7b590",
   "metadata": {},
   "source": [
    "## Tercer modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637024ea",
   "metadata": {},
   "source": [
    "Redes neuronales: Horsepower vs MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_horsepower = MLPRegressor(\n",
    "    max_iter=10000,\n",
    "    hidden_layer_sizes=(64, 64),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate_init=0.001,\n",
    "    validation_fraction=0.2,\n",
    "    early_stopping=True,\n",
    "    random_state=0,\n",
    ")\n",
    "mlp_horsepower.fit(standarized_train_horsepower, train_labels)\n",
    "\n",
    "y = mlp_horsepower.predict(scaled_x)\n",
    "plot_horsepower(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp_horsepower.predict(standarized_test_horsepower)\n",
    "\n",
    "test_results[\"mlp_horsepower\"] = mean_squared_error(\n",
    "    y_true=test_labels,\n",
    "    y_pred=y_pred,\n",
    ")\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b634c55",
   "metadata": {},
   "source": [
    "## Cuarto modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786da7b1",
   "metadata": {},
   "source": [
    "Redes neuronales: variables independientes vs MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cafccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(\n",
    "    max_iter=10000,\n",
    "    hidden_layer_sizes=(64, 64),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate_init=0.001,\n",
    "    validation_fraction=0.2,\n",
    "    early_stopping=True,\n",
    "    random_state=0,\n",
    ")\n",
    "mlp.fit(standarized_train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03334d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = mlp.predict(standarized_test_features)\n",
    "\n",
    "plot_predictions(\n",
    "    y_true=test_labels,\n",
    "    y_pred=test_predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a37be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[\"mlp\"] = mean_squared_error(\n",
    "    y_true=test_labels,\n",
    "    y_pred=test_predictions,\n",
    ")\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c08dc",
   "metadata": {},
   "source": [
    "## Comparación final de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5438be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_results, index=[\"Mean squared error [MPG]\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff73df",
   "metadata": {},
   "source": [
    "## Guardar modelo y escalador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584c396",
   "metadata": {},
   "source": [
    "Es necesario guardar también el escalador que nos permita transformar las variables para que estén en la forma apropiada que espera el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cade07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"mlp.pickle\", \"wb\") as file:\n",
    "    pickle.dump(mlp, file)\n",
    "\n",
    "with open(\"features_scaler.pickle\", \"wb\") as file:\n",
    "    pickle.dump(features_scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8bdff0",
   "metadata": {},
   "source": [
    "# Ejemplo de Uso del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d1fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.744565284379876"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"../files/input/auto_mpg.csv\")\n",
    "dataset = dataset.dropna()\n",
    "dataset[\"Origin\"] = dataset[\"Origin\"].map(\n",
    "    {1: \"USA\", 2: \"Europe\", 3: \"Japan\"},\n",
    ")\n",
    "dataset = pd.get_dummies(dataset, columns=[\"Origin\"], prefix=\"\", prefix_sep=\"\")\n",
    "y_true = dataset.pop(\"MPG\")\n",
    "\n",
    "\n",
    "with open(\"mlp.pickle\", \"rb\") as file:\n",
    "    new_mlp = pickle.load(file)\n",
    "\n",
    "with open(\"features_scaler.pickle\", \"rb\") as file:\n",
    "    new_features_scaler = pickle.load(file)\n",
    "\n",
    "standarized_dataset = new_features_scaler.transform(dataset)\n",
    "y_pred = mlp.predict(standarized_dataset)\n",
    "\n",
    "mean_squared_error(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ec1d5",
   "metadata": {},
   "source": [
    "## 📊 Resumen y Conclusiones del Análisis\n",
    "\n",
    "### 🎯 Objetivos Alcanzados\n",
    "Este notebook presenta un análisis completo de regresión para predecir el consumo de combustible (MPG) de automóviles, implementando las siguientes etapas:\n",
    "\n",
    "1. **Análisis Exploratorio de Datos (EDA)**\n",
    "   - Identificación de patrones y correlaciones\n",
    "   - Detección de valores faltantes y outliers\n",
    "   - Visualización de distribuciones y relaciones\n",
    "\n",
    "2. **Preprocesamiento de Datos**\n",
    "   - Limpieza de datos (eliminación de valores nulos)\n",
    "   - Transformación de variables categóricas (One-Hot Encoding)\n",
    "   - Normalización de variables numéricas (StandardScaler)\n",
    "\n",
    "3. **Desarrollo de Modelos**\n",
    "   - Regresión lineal múltiple\n",
    "   - Comparación de diferentes algoritmos\n",
    "   - Evaluación de rendimiento con métricas apropiadas\n",
    "\n",
    "### 🔍 Hallazgos Clave\n",
    "- **Variables más influyentes**: Peso, desplazamiento y potencia del motor\n",
    "- **Impacto de la normalización**: Mejora significativa en la convergencia del modelo\n",
    "- **Rendimiento del modelo**: [Los resultados específicos se mostrarán al ejecutar las celdas]\n",
    "\n",
    "### 💡 Aplicaciones Prácticas\n",
    "Este tipo de análisis puede ser utilizado para:\n",
    "- **Industria automotriz**: Optimización del diseño de vehículos\n",
    "- **Regulación ambiental**: Establecimiento de estándares de eficiencia\n",
    "- **Consumidores**: Toma de decisiones informadas de compra\n",
    "- **Investigación**: Estudios sobre sostenibilidad y eficiencia energética\n",
    "\n",
    "### ⚠️ Limitaciones y Consideraciones\n",
    "- **Datos históricos**: El modelo se basa en datos de los años 70-80\n",
    "- **Generalización**: Puede no ser aplicable a vehículos modernos\n",
    "- **Variables faltantes**: No se consideraron factores como tecnología híbrida/eléctrica\n",
    "- **Validación cruzada**: Se recomienda implementar k-fold CV para mayor robustez\n",
    "\n",
    "---\n",
    "*Análisis desarrollado como parte del curso de Machine Learning - Regresión Básica*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
