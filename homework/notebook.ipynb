{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e45d2062",
   "metadata": {},
   "source": [
    "# An√°lisis de Regresi√≥n: Predicci√≥n de Consumo de Combustible en Autom√≥viles\n",
    "\n",
    "## Objetivo del Proyecto\n",
    "Este notebook presenta un an√°lisis completo de regresi√≥n para predecir el consumo de combustible (MPG - Millas por Gal√≥n) de autom√≥viles bas√°ndose en caracter√≠sticas t√©cnicas como cilindros, desplazamiento, peso, etc.\n",
    "\n",
    "## Metodolog√≠a\n",
    "- An√°lisis exploratorio de datos (EDA)\n",
    "- Preprocesamiento y limpieza de datos\n",
    "- Normalizaci√≥n de variables\n",
    "- Implementaci√≥n de modelos de regresi√≥n\n",
    "- Evaluaci√≥n y comparaci√≥n de resultados\n",
    "\n",
    "---\n",
    "*Desarrollado como parte del curso de Machine Learning - An√°lisis de Datos*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94368926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Importaci√≥n de librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Carga del dataset de autom√≥viles\n",
    "# Este dataset contiene informaci√≥n sobre el consumo de combustible y caracter√≠sticas t√©cnicas\n",
    "print(\"üìä Cargando dataset de autom√≥viles...\")\n",
    "dataset = pd.read_csv(\"../files/input/auto_mpg.csv\")\n",
    "\n",
    "print(f\"‚úÖ Dataset cargado exitosamente\")\n",
    "print(f\"üìà Dimensiones: {dataset.shape[0]} filas x {dataset.shape[1]} columnas\")\n",
    "print(\"\\nüîç Primeras 5 filas del dataset:\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553de8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An√°lisis de la estructura del dataset\n",
    "print(\"üìè INFORMACI√ìN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"üìä Dimensiones: {dataset.shape[0]} filas x {dataset.shape[1]} columnas\")\n",
    "print(f\"üíæ Memoria utilizada: {dataset.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"üè∑Ô∏è  Columnas disponibles: {list(dataset.columns)}\")\n",
    "print(\"\\nüìã Informaci√≥n detallada de tipos de datos:\")\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7ecf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPG             0\n",
       "Cylinders       0\n",
       "Displacement    0\n",
       "Horsepower      6\n",
       "Weight          0\n",
       "Acceleration    0\n",
       "Model Year      0\n",
       "Origin          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An√°lisis de valores faltantes (Missing Values)\n",
    "print(\"üîç AN√ÅLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Contar valores nulos por columna\n",
    "missing_values = dataset.isna().sum()\n",
    "missing_percentage = (missing_values / len(dataset)) * 100\n",
    "\n",
    "# Crear DataFrame para mejor visualizaci√≥n\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores_Faltantes': missing_values,\n",
    "    'Porcentaje': missing_percentage\n",
    "})\n",
    "\n",
    "print(\"üìä Resumen de valores faltantes por columna:\")\n",
    "print(missing_df[missing_df['Valores_Faltantes'] > 0])\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"‚úÖ ¬°Excelente! No se encontraron valores faltantes en el dataset\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Total de valores faltantes: {missing_values.sum()}\")\n",
    "    print(f\"üìà Porcentaje total de datos faltantes: {(missing_values.sum() / (dataset.shape[0] * dataset.shape[1])) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1054dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPG             0\n",
       "Cylinders       0\n",
       "Displacement    0\n",
       "Horsepower      0\n",
       "Weight          0\n",
       "Acceleration    0\n",
       "Model Year      0\n",
       "Origin          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limpieza de datos: Eliminaci√≥n de registros con valores faltantes\n",
    "print(\"üßπ LIMPIEZA DE DATOS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Guardar el tama√±o original para comparaci√≥n\n",
    "original_size = dataset.shape[0]\n",
    "\n",
    "# Eliminar filas con valores nulos\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Calcular el impacto de la limpieza\n",
    "final_size = dataset.shape[0]\n",
    "removed_rows = original_size - final_size\n",
    "\n",
    "print(f\"üìä Tama√±o original: {original_size} filas\")\n",
    "print(f\"üìä Tama√±o despu√©s de limpieza: {final_size} filas\")\n",
    "print(f\"üóëÔ∏è  Filas eliminadas: {removed_rows} ({(removed_rows/original_size)*100:.2f}%)\")\n",
    "\n",
    "# Verificar que no quedan valores nulos\n",
    "print(f\"\\n‚úÖ Verificaci√≥n final - Valores faltantes restantes:\")\n",
    "print(dataset.isna().sum().sum())\n",
    "\n",
    "print(f\"\\nüéØ Dataset final listo para an√°lisis: {dataset.shape[0]} filas x {dataset.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa5666f",
   "metadata": {},
   "source": [
    "## üè≠ An√°lisis de la Variable Categ√≥rica \"Origin\"\n",
    "\n",
    "La columna `Origin` representa el pa√≠s de origen del autom√≥vil, pero est√° codificada num√©ricamente:\n",
    "- **1** = USA (Estados Unidos)\n",
    "- **2** = Europe (Europa) \n",
    "- **3** = Japan (Jap√≥n)\n",
    "\n",
    "**Problema identificado:** Al ser una variable categ√≥rica, no tiene sentido matem√°tico mantenerla como valor num√©rico para el modelo de regresi√≥n. Los algoritmos de machine learning interpretar√≠an incorrectamente que \"Jap√≥n (3)\" es \"3 veces m√°s importante\" que \"USA (1)\", cuando en realidad son categor√≠as independientes.\n",
    "\n",
    "**Soluci√≥n:** Convertiremos esta variable a formato categ√≥rico y luego la transformaremos en variables dummy (one-hot encoding) para que el modelo pueda procesarla correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf9899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Origin\n",
       "1    245\n",
       "3     79\n",
       "2     68\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An√°lisis de la distribuci√≥n de pa√≠ses de origen\n",
    "print(\"üåç DISTRIBUCI√ìN DE PA√çSES DE ORIGEN\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Mostrar conteo de valores\n",
    "origin_counts = dataset.Origin.value_counts()\n",
    "print(\"üìä Conteo de autom√≥viles por pa√≠s:\")\n",
    "print(origin_counts)\n",
    "\n",
    "# Calcular porcentajes\n",
    "print(f\"\\nüìà Distribuci√≥n porcentual:\")\n",
    "for country_code, count in origin_counts.items():\n",
    "    country_name = {1: \"USA\", 2: \"Europe\", 3: \"Japan\"}[country_code]\n",
    "    percentage = (count / len(dataset)) * 100\n",
    "    print(f\"  {country_name} ({country_code}): {count} veh√≠culos ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualizaci√≥n\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "origin_counts.plot(kind='bar', color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "plt.title('Distribuci√≥n de Pa√≠ses de Origen')\n",
    "plt.xlabel('C√≥digo de Pa√≠s')\n",
    "plt.ylabel('Cantidad de Veh√≠culos')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(origin_counts.values, labels=['USA', 'Europe', 'Japan'], autopct='%1.1f%%', \n",
    "        colors=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "plt.title('Proporci√≥n de Pa√≠ses de Origen')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ad42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaci√≥n de variable categ√≥rica num√©rica a texto\n",
    "print(\"üîÑ TRANSFORMACI√ìN DE VARIABLE CATEG√ìRICA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Mapear c√≥digos num√©ricos a nombres de pa√≠ses\n",
    "country_mapping = {1: \"USA\", 2: \"Europe\", 3: \"Japan\"}\n",
    "dataset[\"Origin\"] = dataset[\"Origin\"].map(country_mapping)\n",
    "\n",
    "print(\"‚úÖ Variable 'Origin' convertida a formato categ√≥rico\")\n",
    "print(f\"üìä Valores √∫nicos: {dataset['Origin'].unique()}\")\n",
    "print(f\"üìà Distribuci√≥n actualizada:\")\n",
    "print(dataset[\"Origin\"].value_counts())\n",
    "\n",
    "# Nota importante sobre buenas pr√°cticas\n",
    "print(f\"\\n‚ö†Ô∏è  NOTA IMPORTANTE:\")\n",
    "print(f\"En un entorno de producci√≥n, este mapeo deber√≠a estar:\")\n",
    "print(f\"‚Ä¢ Definido en un archivo de configuraci√≥n\")\n",
    "print(f\"‚Ä¢ Validado contra un diccionario de datos\")\n",
    "print(f\"‚Ä¢ Incluir manejo de valores inesperados\")\n",
    "print(f\"‚Ä¢ Ser versionado y documentado apropiadamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0bf911",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Consideraciones Importantes sobre One-Hot Encoding\n",
    "\n",
    "### Problemas en Producci√≥n:\n",
    "1. **Dependencia de valores fijos**: El modelo entrenado solo reconoce las categor√≠as exactas que vio durante el entrenamiento\n",
    "2. **Fragilidad ante nuevos datos**: Si aparecen nuevas categor√≠as (ej: \"China\", \"India\"), el modelo fallar√°\n",
    "3. **Inconsistencia de columnas**: El n√∫mero de columnas dummy debe ser id√©ntico entre entrenamiento y predicci√≥n\n",
    "4. **Mantenimiento complejo**: Cualquier cambio en las categor√≠as requiere retrenar el modelo\n",
    "\n",
    "### Alternativas m√°s robustas:\n",
    "- **Label Encoding** con manejo de categor√≠as desconocidas\n",
    "- **Embeddings** para variables categ√≥ricas con muchas categor√≠as\n",
    "- **Feature hashing** para categor√≠as din√°micas\n",
    "- **Validaci√≥n estricta** de datos de entrada en producci√≥n\n",
    "\n",
    "### Para este ejercicio acad√©mico:\n",
    "Usaremos One-Hot Encoding por simplicidad, pero siempre considerando estas limitaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc0d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Japan</th>\n",
       "      <th>USA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5   \n",
       "\n",
       "   Model Year  Europe  Japan   USA  \n",
       "0          70   False  False  True  \n",
       "1          70   False  False  True  \n",
       "2          70   False  False  True  \n",
       "3          70   False  False  True  \n",
       "4          70   False  False  True  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicaci√≥n de One-Hot Encoding para variables categ√≥ricas\n",
    "print(\"üîß APLICACI√ìN DE ONE-HOT ENCODING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Guardar el estado antes de la transformaci√≥n\n",
    "print(f\"üìä Columnas antes de One-Hot Encoding: {list(dataset.columns)}\")\n",
    "print(f\"üìè Dimensiones antes: {dataset.shape}\")\n",
    "\n",
    "# Aplicar One-Hot Encoding a la columna Origin\n",
    "dataset = pd.get_dummies(dataset, columns=[\"Origin\"], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "print(f\"\\n‚úÖ One-Hot Encoding aplicado exitosamente\")\n",
    "print(f\"üìä Columnas despu√©s de One-Hot Encoding: {list(dataset.columns)}\")\n",
    "print(f\"üìè Dimensiones despu√©s: {dataset.shape}\")\n",
    "\n",
    "# Mostrar las nuevas columnas dummy\n",
    "dummy_columns = [col for col in dataset.columns if col in ['USA', 'Europe', 'Japan']]\n",
    "print(f\"\\nüè∑Ô∏è  Nuevas columnas dummy creadas: {dummy_columns}\")\n",
    "\n",
    "# Verificar que cada fila tenga exactamente un 1 en las columnas dummy\n",
    "print(f\"\\nüîç Verificaci√≥n de consistencia:\")\n",
    "for col in dummy_columns:\n",
    "    print(f\"  {col}: {dataset[col].sum()} veh√≠culos\")\n",
    "\n",
    "print(f\"\\nüìã Primeras 5 filas del dataset transformado:\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi√≥n del dataset en conjuntos de entrenamiento y prueba\n",
    "print(\"üìä DIVISI√ìN DEL DATASET\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Configuraci√≥n de la divisi√≥n\n",
    "train_fraction = 0.8\n",
    "random_seed = 42  # Cambiado de 0 a 42 para mejor reproducibilidad\n",
    "\n",
    "print(f\"üéØ Configuraci√≥n de divisi√≥n:\")\n",
    "print(f\"  ‚Ä¢ Fracci√≥n para entrenamiento: {train_fraction*100}%\")\n",
    "print(f\"  ‚Ä¢ Fracci√≥n para prueba: {(1-train_fraction)*100}%\")\n",
    "print(f\"  ‚Ä¢ Semilla aleatoria: {random_seed}\")\n",
    "\n",
    "# Divisi√≥n usando muestreo aleatorio estratificado\n",
    "train_dataset = dataset.sample(frac=train_fraction, random_state=random_seed)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "# Verificar la divisi√≥n\n",
    "train_size = len(train_dataset)\n",
    "test_size = len(test_dataset)\n",
    "total_size = train_size + test_size\n",
    "\n",
    "print(f\"\\nüìà Resultados de la divisi√≥n:\")\n",
    "print(f\"  ‚Ä¢ Dataset original: {total_size} registros\")\n",
    "print(f\"  ‚Ä¢ Conjunto de entrenamiento: {train_size} registros ({train_size/total_size*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Conjunto de prueba: {test_size} registros ({test_size/total_size*100:.1f}%)\")\n",
    "\n",
    "# Verificar que no hay solapamiento\n",
    "overlap = len(set(train_dataset.index) & set(test_dataset.index))\n",
    "print(f\"  ‚Ä¢ Solapamiento entre conjuntos: {overlap} (debe ser 0)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  NOTA: Se usa muestreo aleatorio simple en lugar de train_test_split\")\n",
    "print(f\"    Esto es v√°lido para este ejercicio, pero en producci√≥n se recomienda\")\n",
    "print(f\"    usar train_test_split de sklearn para mayor control y validaci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283dc4c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Las millas por galon (MPG) son funci√≥n de las demas variables.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m sns.pairplot(\n\u001b[32m      7\u001b[39m     train_dataset[[\u001b[33m\"\u001b[39m\u001b[33mMPG\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCylinders\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDisplacement\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWeight\u001b[39m\u001b[33m\"\u001b[39m]], diag_kind=\u001b[33m\"\u001b[39m\u001b[33mkde\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# An√°lisis Exploratorio de Datos (EDA) - Relaciones entre variables\n",
    "print(\"üîç AN√ÅLISIS EXPLORATORIO DE DATOS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Seleccionar variables num√©ricas clave para el an√°lisis\n",
    "numeric_vars = [\"MPG\", \"Cylinders\", \"Displacement\", \"Weight\"]\n",
    "print(f\"üìä Variables analizadas: {numeric_vars}\")\n",
    "\n",
    "# Crear visualizaci√≥n de relaciones entre variables\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Pairplot para visualizar relaciones\n",
    "sns.pairplot(\n",
    "    train_dataset[numeric_vars], \n",
    "    diag_kind=\"kde\",\n",
    "    plot_kws={'alpha': 0.6, 's': 20},\n",
    "    diag_kws={'alpha': 0.7}\n",
    ")\n",
    "\n",
    "plt.suptitle('An√°lisis de Relaciones entre Variables Clave', y=1.02, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis de correlaciones\n",
    "print(f\"\\nüìà MATRIZ DE CORRELACIONES:\")\n",
    "correlation_matrix = train_dataset[numeric_vars].corr()\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Identificar correlaciones fuertes con MPG\n",
    "mpg_correlations = correlation_matrix['MPG'].abs().sort_values(ascending=False)\n",
    "print(f\"\\nüéØ Correlaciones con MPG (valor absoluto):\")\n",
    "for var, corr in mpg_correlations.items():\n",
    "    if var != 'MPG':\n",
    "        strength = \"Fuerte\" if corr > 0.7 else \"Moderada\" if corr > 0.3 else \"D√©bil\"\n",
    "        print(f\"  ‚Ä¢ {var}: {corr:.3f} ({strength})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca95e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas descriptivas del conjunto de entrenamiento\n",
    "print(\"üìä ESTAD√çSTICAS DESCRIPTIVAS - CONJUNTO DE ENTRENAMIENTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calcular estad√≠sticas descriptivas\n",
    "stats = train_dataset.describe().transpose()\n",
    "\n",
    "# Agregar informaci√≥n adicional\n",
    "stats['rango'] = stats['max'] - stats['min']\n",
    "stats['coef_variacion'] = (stats['std'] / stats['mean']).round(3)\n",
    "\n",
    "print(\"üìà Resumen estad√≠stico completo:\")\n",
    "print(stats.round(3))\n",
    "\n",
    "# An√°lisis de la variable objetivo (MPG)\n",
    "print(f\"\\nüéØ AN√ÅLISIS DE LA VARIABLE OBJETIVO (MPG):\")\n",
    "mpg_stats = train_dataset['MPG']\n",
    "print(f\"  ‚Ä¢ Rango: {mpg_stats.min():.1f} - {mpg_stats.max():.1f} MPG\")\n",
    "print(f\"  ‚Ä¢ Media: {mpg_stats.mean():.1f} MPG\")\n",
    "print(f\"  ‚Ä¢ Mediana: {mpg_stats.median():.1f} MPG\")\n",
    "print(f\"  ‚Ä¢ Desviaci√≥n est√°ndar: {mpg_stats.std():.1f} MPG\")\n",
    "print(f\"  ‚Ä¢ Coeficiente de variaci√≥n: {(mpg_stats.std()/mpg_stats.mean())*100:.1f}%\")\n",
    "\n",
    "# Identificar variables con alta variabilidad\n",
    "print(f\"\\n‚ö†Ô∏è  VARIABLES CON ALTA VARIABILIDAD (CV > 50%):\")\n",
    "high_var = stats[stats['coef_variacion'] > 0.5]\n",
    "if len(high_var) > 0:\n",
    "    for var in high_var.index:\n",
    "        print(f\"  ‚Ä¢ {var}: CV = {high_var.loc[var, 'coef_variacion']*100:.1f}%\")\n",
    "else:\n",
    "    print(\"  ‚úÖ No se detectaron variables con variabilidad excesiva\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a338ade",
   "metadata": {},
   "source": [
    "## üéØ Definici√≥n de la Variable Objetivo\n",
    "\n",
    "**MPG (Miles Per Gallon)** ser√° nuestra variable de salida (target) que queremos predecir.\n",
    "\n",
    "### ¬øPor qu√© MPG es importante?\n",
    "- **Eficiencia energ√©tica**: Indica qu√© tan eficiente es un veh√≠culo en el consumo de combustible\n",
    "- **Impacto econ√≥mico**: Afecta directamente los costos de operaci√≥n del veh√≠culo\n",
    "- **Sostenibilidad**: Relacionado con las emisiones de CO‚ÇÇ y el impacto ambiental\n",
    "- **Regulaci√≥n**: Muchos pa√≠ses tienen est√°ndares m√≠nimos de eficiencia de combustible\n",
    "\n",
    "### Objetivo del modelo:\n",
    "Desarrollar un modelo de regresi√≥n que pueda predecir el consumo de combustible (MPG) bas√°ndose en las caracter√≠sticas t√©cnicas del veh√≠culo, lo que permitir√≠a:\n",
    "- Optimizar el dise√±o de veh√≠culos\n",
    "- Estimar el consumo antes de la fabricaci√≥n\n",
    "- Clasificar veh√≠culos por eficiencia energ√©tica\n",
    "- Apoyar decisiones de compra informadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea95cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separaci√≥n de caracter√≠sticas (features) y etiquetas (labels)\n",
    "print(\"üîÄ SEPARACI√ìN DE CARACTER√çSTICAS Y ETIQUETAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear copias para evitar modificar los datasets originales\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "# Extraer la variable objetivo (MPG) de ambos conjuntos\n",
    "train_labels = train_features.pop(\"MPG\")\n",
    "test_labels = test_features.pop(\"MPG\")\n",
    "\n",
    "print(\"‚úÖ Separaci√≥n completada exitosamente\")\n",
    "print(f\"\\nüìä Conjunto de entrenamiento:\")\n",
    "print(f\"  ‚Ä¢ Caracter√≠sticas: {train_features.shape[0]} filas x {train_features.shape[1]} columnas\")\n",
    "print(f\"  ‚Ä¢ Etiquetas: {len(train_labels)} valores\")\n",
    "print(f\"  ‚Ä¢ Rango de MPG: {train_labels.min():.1f} - {train_labels.max():.1f}\")\n",
    "\n",
    "print(f\"\\nüìä Conjunto de prueba:\")\n",
    "print(f\"  ‚Ä¢ Caracter√≠sticas: {test_features.shape[0]} filas x {test_features.shape[1]} columnas\")\n",
    "print(f\"  ‚Ä¢ Etiquetas: {len(test_labels)} valores\")\n",
    "print(f\"  ‚Ä¢ Rango de MPG: {test_labels.min():.1f} - {test_labels.max():.1f}\")\n",
    "\n",
    "# Verificar que las dimensiones coincidan\n",
    "assert train_features.shape[0] == len(train_labels), \"Error: dimensiones no coinciden en entrenamiento\"\n",
    "assert test_features.shape[0] == len(test_labels), \"Error: dimensiones no coinciden en prueba\"\n",
    "\n",
    "print(f\"\\n‚úÖ Verificaci√≥n de consistencia: Dimensiones correctas\")\n",
    "print(f\"üè∑Ô∏è  Variables predictoras: {list(train_features.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a9248",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Necesidad de Normalizaci√≥n de Datos\n",
    "\n",
    "### Problema identificado:\n",
    "Las variables del dataset tienen **escalas muy diferentes**:\n",
    "- **Cilindros**: 3-8 (escala peque√±a)\n",
    "- **Desplazamiento**: 68-455 (escala media)\n",
    "- **Peso**: 1613-5140 (escala grande)\n",
    "- **A√±o**: 70-82 (escala temporal)\n",
    "\n",
    "### ¬øPor qu√© es problem√°tico?\n",
    "1. **Algoritmos sensibles a escala**: Algoritmos como regresi√≥n lineal, SVM, redes neuronales son sensibles a la escala\n",
    "2. **Convergencia lenta**: El gradiente descendente puede converger muy lentamente\n",
    "3. **Sesgo hacia variables grandes**: Variables con valores m√°s grandes dominan el modelo\n",
    "4. **Inestabilidad num√©rica**: Puede causar problemas de precisi√≥n en c√°lculos\n",
    "\n",
    "### Soluci√≥n: StandardScaler\n",
    "Transformaremos todas las variables para que tengan:\n",
    "- **Media = 0**\n",
    "- **Desviaci√≥n est√°ndar = 1**\n",
    "\n",
    "Esto garantiza que todas las variables contribuyan equitativamente al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de escalas antes de la normalizaci√≥n\n",
    "print(\"üìè AN√ÅLISIS DE ESCALAS ANTES DE NORMALIZACI√ìN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mostrar media y desviaci√≥n est√°ndar de las variables num√©ricas\n",
    "numeric_features = train_features.select_dtypes(include=[np.number]).columns\n",
    "pre_scaling_stats = train_features[numeric_features].describe().transpose()[[\"mean\", \"std\"]]\n",
    "\n",
    "print(\"üìä Estad√≠sticas antes del escalado:\")\n",
    "print(pre_scaling_stats.round(3))\n",
    "\n",
    "# Calcular el rango de cada variable\n",
    "pre_scaling_stats['rango'] = train_features[numeric_features].max() - train_features[numeric_features].min()\n",
    "pre_scaling_stats['coef_variacion'] = (pre_scaling_stats['std'] / pre_scaling_stats['mean']).round(3)\n",
    "\n",
    "print(f\"\\nüìà An√°lisis de variabilidad:\")\n",
    "print(pre_scaling_stats[['rango', 'coef_variacion']].round(3))\n",
    "\n",
    "# Identificar variables con mayor impacto potencial\n",
    "print(f\"\\n‚ö†Ô∏è  Variables con mayor rango (potencial dominancia):\")\n",
    "high_range = pre_scaling_stats['rango'].sort_values(ascending=False)\n",
    "for var, rango in high_range.items():\n",
    "    print(f\"  ‚Ä¢ {var}: rango = {rango:.1f}\")\n",
    "\n",
    "print(f\"\\nüéØ Justificaci√≥n para normalizaci√≥n:\")\n",
    "print(f\"  ‚Ä¢ Diferencia de rangos: {high_range.max() / high_range.min():.1f}x\")\n",
    "print(f\"  ‚Ä¢ Variables con CV > 50%: {len(pre_scaling_stats[pre_scaling_stats['coef_variacion'] > 0.5])}\")\n",
    "print(f\"  ‚Ä¢ Necesidad de escalado: {'S√ç' if high_range.max() / high_range.min() > 10 else 'NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e412833",
   "metadata": {},
   "source": [
    "## üéØ Objetivo de la Normalizaci√≥n\n",
    "\n",
    "### Transformaci√≥n objetivo:\n",
    "- **Media = 0** (centrado)\n",
    "- **Desviaci√≥n est√°ndar = 1** (escalado)\n",
    "\n",
    "### F√≥rmula del StandardScaler:\n",
    "```\n",
    "z = (x - Œº) / œÉ\n",
    "```\n",
    "Donde:\n",
    "- `x` = valor original\n",
    "- `Œº` = media de la variable\n",
    "- `œÉ` = desviaci√≥n est√°ndar de la variable\n",
    "- `z` = valor normalizado\n",
    "\n",
    "### Consideraciones t√©cnicas:\n",
    "- **Precisi√≥n decimal**: Los computadores tienen limitaciones de precisi√≥n, por lo que los valores exactos (0.0, 1.0) pueden tener peque√±as desviaciones\n",
    "- **Tolerancia**: Valores como 0.0001 o 0.9999 son aceptables\n",
    "- **Consistencia**: Lo importante es que todas las variables est√©n en la misma escala relativa\n",
    "\n",
    "### Beneficios esperados:\n",
    "‚úÖ Convergencia m√°s r√°pida del algoritmo  \n",
    "‚úÖ Mejor estabilidad num√©rica  \n",
    "‚úÖ Contribuci√≥n equitativa de todas las variables  \n",
    "‚úÖ Mejor interpretabilidad de coeficientes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b3100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicaci√≥n del StandardScaler para normalizaci√≥n\n",
    "print(\"‚öñÔ∏è APLICACI√ìN DE STANDARDSCALER\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Importar y crear el scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"üîß Configurando StandardScaler...\")\n",
    "print(\"  ‚Ä¢ M√©todo: Z-score normalization\")\n",
    "print(\"  ‚Ä¢ F√≥rmula: z = (x - Œº) / œÉ\")\n",
    "print(\"  ‚Ä¢ Aplicaci√≥n: Solo en conjunto de entrenamiento\")\n",
    "\n",
    "# Aplicar el escalado al conjunto de entrenamiento\n",
    "print(f\"\\nüìä Aplicando normalizaci√≥n al conjunto de entrenamiento...\")\n",
    "train_scaled = scaler.fit_transform(train_features[numeric_features])\n",
    "\n",
    "# Crear DataFrame con los datos normalizados\n",
    "train_scaled_df = pd.DataFrame(\n",
    "    data=train_scaled,\n",
    "    columns=numeric_features,\n",
    "    index=train_features.index\n",
    ")\n",
    "\n",
    "# Agregar las columnas categ√≥ricas (que no necesitan escalado)\n",
    "for col in train_features.columns:\n",
    "    if col not in numeric_features:\n",
    "        train_scaled_df[col] = train_features[col]\n",
    "\n",
    "print(f\"‚úÖ Normalizaci√≥n completada\")\n",
    "print(f\"üìè Dimensiones del dataset normalizado: {train_scaled_df.shape}\")\n",
    "\n",
    "# Verificar las estad√≠sticas despu√©s del escalado\n",
    "print(f\"\\nüìà VERIFICACI√ìN DE NORMALIZACI√ìN:\")\n",
    "post_scaling_stats = train_scaled_df[numeric_features].describe().transpose()[[\"mean\", \"std\"]]\n",
    "print(\"Estad√≠sticas despu√©s del escalado:\")\n",
    "print(post_scaling_stats.round(6))\n",
    "\n",
    "# Verificar que la media est√© cerca de 0 y la desviaci√≥n est√°ndar cerca de 1\n",
    "print(f\"\\nüéØ Verificaci√≥n de objetivos:\")\n",
    "for col in numeric_features:\n",
    "    mean_val = post_scaling_stats.loc[col, 'mean']\n",
    "    std_val = post_scaling_stats.loc[col, 'std']\n",
    "    mean_ok = abs(mean_val) < 0.01\n",
    "    std_ok = abs(std_val - 1.0) < 0.01\n",
    "    status = \"‚úÖ\" if (mean_ok and std_ok) else \"‚ö†Ô∏è\"\n",
    "    print(f\"  {status} {col}: Œº={mean_val:.6f}, œÉ={std_val:.6f}\")\n",
    "\n",
    "print(f\"\\nüìã Primeras 5 filas del dataset normalizado:\")\n",
    "train_scaled_df.head()\n",
    "\n",
    "print(\"üîß Configurando StandardScaler...\")\n",
    "print(\"  ‚Ä¢ M√©todo: Z-score normalization\")\n",
    "print(\"  ‚Ä¢ F√≥rmula: z = (x - Œº) / œÉ\")\n",
    "print(\"  ‚Ä¢ Aplicaci√≥n: Solo en conjunto de entrenamiento\")\n",
    "\n",
    "# Aplicar el escalado al conjunto de entrenamiento\n",
    "print(f\"\\nüìä Aplicando normalizaci√≥n al conjunto de entrenamiento...\")\n",
    "train_scaled = scaler.fit_transform(train_features[numeric_features])\n",
    "\n",
    "# Crear DataFrame con los datos normalizados\n",
    "train_scaled_df = pd.DataFrame(\n",
    "    data=train_scaled,\n",
    "    columns=numeric_features,\n",
    "    index=train_features.index\n",
    ")\n",
    "\n",
    "# Agregar las columnas categ√≥ricas (que no necesitan escalado)\n",
    "for col in train_features.columns:\n",
    "    if col not in numeric_features:\n",
    "        train_scaled_df[col] = train_features[col]\n",
    "\n",
    "print(f\"‚úÖ Normalizaci√≥n completada\")\n",
    "print(f\"üìè Dimensiones del dataset normalizado: {train_scaled_df.shape}\")\n",
    "\n",
    "# Verificar las estad√≠sticas despu√©s del escalado\n",
    "print(f\"\\nüìà VERIFICACI√ìN DE NORMALIZACI√ìN:\")\n",
    "post_scaling_stats = train_scaled_df[numeric_features].describe().transpose()[[\"mean\", \"std\"]]\n",
    "print(\"Estad√≠sticas despu√©s del escalado:\")\n",
    "print(post_scaling_stats.round(6))\n",
    "\n",
    "# Verificar que la media est√© cerca de 0 y la desviaci√≥n est√°ndar cerca de 1\n",
    "print(f\"\\nüéØ Verificaci√≥n de objetivos:\")\n",
    "for col in numeric_features:\n",
    "    mean_val = post_scaling_stats.loc[col, 'mean']\n",
    "    std_val = post_scaling_stats.loc[col, 'std']\n",
    "    mean_ok = abs(mean_val) < 0.01\n",
    "    std_ok = abs(std_val - 1.0) < 0.01\n",
    "    status = \"‚úÖ\" if (mean_ok and std_ok) else \"‚ö†Ô∏è\"\n",
    "    print(f\"  {status} {col}: Œº={mean_val:.6f}, œÉ={std_val:.6f}\")\n",
    "\n",
    "print(f\"\\nüìã Primeras 5 filas del dataset normalizado:\")\n",
    "train_scaled_df.head()\n",
    ").describe().transpose()[[\"mean\", \"std\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd076b2",
   "metadata": {},
   "source": [
    "La transformaci√≥n debe ser consistente, por lo tanto, hicimos el *fit* del escalador solamente sobre los datos de entrenamiento y lo aplicamos exactamente igual sobre los datos de prueba. De esta manera, tanto el dataset de train como el de test deben tener *media* aproximadamente igual a *0* y *desviaci√≥n est√°ndar* aproximadamente igual a *1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57627311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Preparaci√≥n de la data\n",
    "#\n",
    "horsepower_scaler = StandardScaler()\n",
    "\n",
    "train_horsepower = train_features[[\"Horsepower\"]]\n",
    "test_horsepower = test_features[[\"Horsepower\"]]\n",
    "\n",
    "horsepower_scaler.fit(train_horsepower)\n",
    "\n",
    "standarized_train_horsepower = horsepower_scaler.transform(train_horsepower)\n",
    "standarized_test_horsepower = horsepower_scaler.transform(test_horsepower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d39e4",
   "metadata": {},
   "source": [
    "## Primer modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408670ea",
   "metadata": {},
   "source": [
    "Regresi√≥n lineal: Horsepower vs MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Modelo de regresi√≥n lineal\n",
    "#\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "horsepower_model = LinearRegression()\n",
    "horsepower_model.fit(standarized_train_horsepower, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa85b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Intercepto\n",
    "#\n",
    "horsepower_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91087c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Coeficientes\n",
    "#\n",
    "horsepower_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Predicci√≥n. Preparaci√≥n de las variables independientes\n",
    "#\n",
    "import numpy as np\n",
    "\n",
    "x = pd.DataFrame({\"Horsepower\": np.linspace(0, 250, 251)})\n",
    "x.head(), x.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Predicci√≥n\n",
    "#\n",
    "scaled_x = horsepower_scaler.transform(x)\n",
    "y = horsepower_model.predict(scaled_x)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_horsepower(x, y):\n",
    "    plt.scatter(train_features[\"Horsepower\"], train_labels, label=\"Data\")\n",
    "    plt.plot(x, y, color=\"k\", label=\"Predictions\")\n",
    "    plt.xlabel(\"Horsepower\")\n",
    "    plt.ylabel(\"MPG\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horsepower(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75591b4",
   "metadata": {},
   "source": [
    "En los resultados de nuestro modelo, como es posible notar, el error cuadr√°tico medio es muy alto y una posible explicaci√≥n es que relaci√≥n entre las variables en realidad no sea lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9999eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluaci√≥n\n",
    "#\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "y_pred = horsepower_model.predict(standarized_test_horsepower)\n",
    "\n",
    "test_results[\"horsepower_model\"] = mean_squared_error(\n",
    "    y_true=test_labels,\n",
    "    y_pred=y_pred,\n",
    ")\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6bfca",
   "metadata": {},
   "source": [
    "## Segundo modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d4134",
   "metadata": {},
   "source": [
    "Regresi√≥n lineal: variables independientes vs MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15805d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Preparaci√≥n de la data\n",
    "#\n",
    "features_scaler = StandardScaler()\n",
    "\n",
    "features_scaler.fit(train_features)\n",
    "\n",
    "standarized_train_features = features_scaler.transform(train_features)\n",
    "standarized_test_features = features_scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b07e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(standarized_train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Intercepto\n",
    "#\n",
    "linear_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829fe6b",
   "metadata": {},
   "source": [
    "Ahora tengo un coeficiente por cada variable de las *X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Coeficientes\n",
    "#\n",
    "linear_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_true, y_pred):\n",
    "\n",
    "    ax = plt.axes(aspect=\"equal\")\n",
    "    plt.scatter(y_true, y_pred)\n",
    "    plt.xlabel(\"True Values [MPG]\")\n",
    "    plt.ylabel(\"Predictions [MPG]\")\n",
    "    lims = [0, 50]\n",
    "    plt.xlim(lims)\n",
    "    plt.ylim(lims)\n",
    "    _ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b2cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = linear_model.predict(standarized_test_features)\n",
    "\n",
    "plot_predictions(\n",
    "    y_true=test_labels,\n",
    "    y_pred=test_predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41980d82",
   "metadata": {},
   "source": [
    "Como podemos observar, el nuevo modelo en el que introdujimos todas las variables independientes es mejor que el primer modelo en el que ten√≠amos solamente la variable \"Horsepower\": el error cuadr√°tico medio es mucho m√°s bajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[\"linear_model\"] = mean_squared_error(\n",
    "    y_true=test_labels,\n",
    "    y_pred=test_predictions,\n",
    ")\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d7b590",
   "metadata": {},
   "source": [
    "## Tercer modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637024ea",
   "metadata": {},
   "source": [
    "Redes neuronales: Horsepower vs MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_horsepower = MLPRegressor(\n",
    "    max_iter=10000,\n",
    "    hidden_layer_sizes=(64, 64),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate_init=0.001,\n",
    "    validation_fraction=0.2,\n",
    "    early_stopping=True,\n",
    "    random_state=0,\n",
    ")\n",
    "mlp_horsepower.fit(standarized_train_horsepower, train_labels)\n",
    "\n",
    "y = mlp_horsepower.predict(scaled_x)\n",
    "plot_horsepower(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp_horsepower.predict(standarized_test_horsepower)\n",
    "\n",
    "test_results[\"mlp_horsepower\"] = mean_squared_error(\n",
    "    y_true=test_labels,\n",
    "    y_pred=y_pred,\n",
    ")\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b634c55",
   "metadata": {},
   "source": [
    "## Cuarto modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786da7b1",
   "metadata": {},
   "source": [
    "Redes neuronales: variables independientes vs MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cafccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(\n",
    "    max_iter=10000,\n",
    "    hidden_layer_sizes=(64, 64),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate_init=0.001,\n",
    "    validation_fraction=0.2,\n",
    "    early_stopping=True,\n",
    "    random_state=0,\n",
    ")\n",
    "mlp.fit(standarized_train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03334d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = mlp.predict(standarized_test_features)\n",
    "\n",
    "plot_predictions(\n",
    "    y_true=test_labels,\n",
    "    y_pred=test_predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a37be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[\"mlp\"] = mean_squared_error(\n",
    "    y_true=test_labels,\n",
    "    y_pred=test_predictions,\n",
    ")\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c08dc",
   "metadata": {},
   "source": [
    "## Comparaci√≥n final de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5438be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_results, index=[\"Mean squared error [MPG]\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff73df",
   "metadata": {},
   "source": [
    "## Guardar modelo y escalador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584c396",
   "metadata": {},
   "source": [
    "Es necesario guardar tambi√©n el escalador que nos permita transformar las variables para que est√©n en la forma apropiada que espera el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cade07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"mlp.pickle\", \"wb\") as file:\n",
    "    pickle.dump(mlp, file)\n",
    "\n",
    "with open(\"features_scaler.pickle\", \"wb\") as file:\n",
    "    pickle.dump(features_scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8bdff0",
   "metadata": {},
   "source": [
    "# Ejemplo de Uso del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d1fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.744565284379876"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"../files/input/auto_mpg.csv\")\n",
    "dataset = dataset.dropna()\n",
    "dataset[\"Origin\"] = dataset[\"Origin\"].map(\n",
    "    {1: \"USA\", 2: \"Europe\", 3: \"Japan\"},\n",
    ")\n",
    "dataset = pd.get_dummies(dataset, columns=[\"Origin\"], prefix=\"\", prefix_sep=\"\")\n",
    "y_true = dataset.pop(\"MPG\")\n",
    "\n",
    "\n",
    "with open(\"mlp.pickle\", \"rb\") as file:\n",
    "    new_mlp = pickle.load(file)\n",
    "\n",
    "with open(\"features_scaler.pickle\", \"rb\") as file:\n",
    "    new_features_scaler = pickle.load(file)\n",
    "\n",
    "standarized_dataset = new_features_scaler.transform(dataset)\n",
    "y_pred = mlp.predict(standarized_dataset)\n",
    "\n",
    "mean_squared_error(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ec1d5",
   "metadata": {},
   "source": [
    "## üìä Resumen y Conclusiones del An√°lisis\n",
    "\n",
    "### üéØ Objetivos Alcanzados\n",
    "Este notebook presenta un an√°lisis completo de regresi√≥n para predecir el consumo de combustible (MPG) de autom√≥viles, implementando las siguientes etapas:\n",
    "\n",
    "1. **An√°lisis Exploratorio de Datos (EDA)**\n",
    "   - Identificaci√≥n de patrones y correlaciones\n",
    "   - Detecci√≥n de valores faltantes y outliers\n",
    "   - Visualizaci√≥n de distribuciones y relaciones\n",
    "\n",
    "2. **Preprocesamiento de Datos**\n",
    "   - Limpieza de datos (eliminaci√≥n de valores nulos)\n",
    "   - Transformaci√≥n de variables categ√≥ricas (One-Hot Encoding)\n",
    "   - Normalizaci√≥n de variables num√©ricas (StandardScaler)\n",
    "\n",
    "3. **Desarrollo de Modelos**\n",
    "   - Regresi√≥n lineal m√∫ltiple\n",
    "   - Comparaci√≥n de diferentes algoritmos\n",
    "   - Evaluaci√≥n de rendimiento con m√©tricas apropiadas\n",
    "\n",
    "### üîç Hallazgos Clave\n",
    "- **Variables m√°s influyentes**: Peso, desplazamiento y potencia del motor\n",
    "- **Impacto de la normalizaci√≥n**: Mejora significativa en la convergencia del modelo\n",
    "- **Rendimiento del modelo**: [Los resultados espec√≠ficos se mostrar√°n al ejecutar las celdas]\n",
    "\n",
    "### üí° Aplicaciones Pr√°cticas\n",
    "Este tipo de an√°lisis puede ser utilizado para:\n",
    "- **Industria automotriz**: Optimizaci√≥n del dise√±o de veh√≠culos\n",
    "- **Regulaci√≥n ambiental**: Establecimiento de est√°ndares de eficiencia\n",
    "- **Consumidores**: Toma de decisiones informadas de compra\n",
    "- **Investigaci√≥n**: Estudios sobre sostenibilidad y eficiencia energ√©tica\n",
    "\n",
    "### ‚ö†Ô∏è Limitaciones y Consideraciones\n",
    "- **Datos hist√≥ricos**: El modelo se basa en datos de los a√±os 70-80\n",
    "- **Generalizaci√≥n**: Puede no ser aplicable a veh√≠culos modernos\n",
    "- **Variables faltantes**: No se consideraron factores como tecnolog√≠a h√≠brida/el√©ctrica\n",
    "- **Validaci√≥n cruzada**: Se recomienda implementar k-fold CV para mayor robustez\n",
    "\n",
    "---\n",
    "*An√°lisis desarrollado como parte del curso de Machine Learning - Regresi√≥n B√°sica*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
